{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb1535c-397c-43c1-b144-a70eb241f542",
   "metadata": {},
   "source": [
    "Extracting VIIRS data (3 satellites, L1 + L2) on the cloud for a space and time of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ec1ddf-cb08-4f47-8151-eaa8d668b21b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import earthaccess\n",
    "import datetime as dt\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import time\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter,DayLocator\n",
    "import os\n",
    "import fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "baea72ae-e1dc-4f61-bdec-d8c442a5f802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#user define global variables\n",
    "\n",
    "ROOT = '/projects/my-public-bucket/'\n",
    "'''\n",
    "#canada pyrocb example\n",
    "NAME = pyrocbs\n",
    "CENTROID = [-121.8, 60.1]\n",
    "START = '2023-07-05' #start date is inclusive\n",
    "END = '2023-07-08' #end date is exclusive\n",
    "'''\n",
    "#southern california fire trio example\n",
    "NAME = 'california' #folder name for outputs\n",
    "CENTROID = [-117.4, 34]\n",
    "START = '2024-09-09' #start date is inclusive\n",
    "END = '2024-09-13' #end date is exclusive\n",
    "\n",
    "BBOX = [CENTROID[0]-0.5, CENTROID[1]-0.5, CENTROID[0]+0.5, CENTROID[1]+0.5] #lon1, lat1, lon2, lat2\n",
    "\n",
    "SENSORS = ['SNPP', 'NOAA20', 'NOAA21'] \n",
    "\n",
    "output_dir = '/projects/my-public-bucket/viirs/' #writing\n",
    "input_dir = '/projects/shared-buckets/coffield/viirs/' #reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293463b6-c229-4664-a870-475873c0fa52",
   "metadata": {},
   "source": [
    "Note on start/end formats: earthaccess.search_data() is supposed to be able to handle datetime objects, but I can only get it to work with strings of the format .strftime('%Y-%m-%d') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea46f4f-de9b-4ad4-868b-35fe254bcb1e",
   "metadata": {},
   "source": [
    "Note on bbox: earthaccess.search_data() can also use \"polygon=\" instead of \"bounding_box=\". See amazon_viirs_extraction.ipynb for an example of using a polygon from a shapely geometry instead of a bounding box (the formatting of the polygon string is very picky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d88c6667-9072-4567-a45d-1c61edf84e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_products = {'SNPP':['VNP03IMG','VNP02IMG','VNP14IMG'], \n",
    "            'NOAA20':['VJ103IMG','VJ102IMG','VJ114IMG'],\n",
    "           'NOAA21':['VJ203IMG','VJ202IMG','VJ214IMG']}\n",
    "\n",
    "products = {s: all_products[s] for s in SENSORS} #subset to sensors of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101472a-f678-4bec-8a07-4c577369b064",
   "metadata": {},
   "source": [
    "<h4>If NOAA21 is included, check whether files have already been downloaded. If not, download them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44003c18-db1e-4991-abe6-ceccb005a10c",
   "metadata": {},
   "source": [
    "Current token from Shane will work until Oct 28, 2024: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bae65b-01f4-40a0-9e91-4c702d303cb0",
   "metadata": {},
   "source": [
    "Bearer eyJ0eXAiOiJKV1QiLCJvcmlnaW4iOiJFYXJ0aGRhdGEgTG9naW4iLCJzaWciOiJlZGxqd3RwdWJrZXlfb3BzIiwiYWxnIjoiUlMyNTYifQ.eyJ0eXBlIjoiVXNlciIsInVpZCI6ImNvZmZpZWxkIiwiZXhwIjoxNzMwMTQ1NjM0LCJpYXQiOjE3MjQ5NjE2MzQsImlzcyI6Imh0dHBzOi8vdXJzLmVhcnRoZGF0YS5uYXNhLmdvdiJ9.0KmD3OdpbCoSJd4f7hxjcs7U9cfkbB9Iq0uoX-MRPO6ZHF83FeDj88VW8q4ulVGONURh4-tv6bG58SZt-MV9InrTaI2a9IaxFIMbfwIk3cxkNtyCoptAt-bihj7TvDuzHTLHc-Sxyflya_nKnxTAtdey5G7hWw6MuLFaWNhcj7IvIzXVeUvFEkdZqc6WdDHnQZtC7NROrTurfLVyzXbmFrAbJI7QR8i1n65j6FnchznnXUNllaNTjEE978QLzzCfjIGb91Btl0p2ovvQIYQ8Kaqq_wD2_SkxZm0-IqdFUOfGM5mdg9cSmWflOgx94h8bpUSVnQnFkIZPx55r8-7MmQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f92e304-4ae4-43cc-a79c-095db6db03d1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOAA21 data already downloaded for 2024-09-09 00:00:00\n",
      "NOAA21 data already downloaded for 2024-09-10 00:00:00\n",
      "NOAA21 data already downloaded for 2024-09-11 00:00:00\n",
      "NOAA21 data already downloaded for 2024-09-12 00:00:00\n"
     ]
    }
   ],
   "source": [
    "if 'NOAA21' in products:\n",
    "    \n",
    "    auth = earthaccess.login()\n",
    "    session = auth.get_session()\n",
    "    token = session.headers['Authorization'] #replace with Shane's token if you don't have access to https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/4014/VJ214IMG/\n",
    "    \n",
    "    start = pd.to_datetime(START)\n",
    "    end = pd.to_datetime(END)\n",
    "    day_range = pd.date_range(start, end, inclusive='left')\n",
    "    \n",
    "    for day in day_range:\n",
    "        filepath = f'{output_dir}/VJ214IMG/{day.year}/{day.timetuple().tm_yday}'\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            print('NOAA21 data already downloaded for', day)\n",
    "        else:\n",
    "            print('downloading NOAA21 for', day)\n",
    "            command = f'wget -e robots=off -m -np -R .html,.tmp -nH --cut-dirs=3 \"https://ladsweb.modaps.eosdis.nasa.gov/archive/allData/4014/VJ214IMG/{day.year}/{day.timetuple().tm_yday}/\" --header \"Authorization: {token}\" -P {output_dir}'\n",
    "            #print(command)\n",
    "            os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4812d30-b40c-4f1f-84a2-cc3e99cc8416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_fsspec = fsspec.filesystem(\"s3\", profile=\"maap-data-reader\") #for direct reader access to LPDAAC\n",
    "\n",
    "pix_lut = pd.read_csv('/projects/shared-buckets/coffield/pix_size_lut.csv', index_col='sample') #view zenith angle lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03085dce-9e6b-4156-a58d-ee11753e916a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNPP -------\n",
      "Granules found: 12\n",
      "Opening 12 granules, approx size: 1.78 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bad21e0f77548c6973059b601d6fe8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32abf6a01e143d0a55a4342f75db10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2354d17c2a493cb718aacd5672df86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 12\n",
      "Opening 12 granules, approx size: 2.11 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d317bc0c7dce410daa715888f1cfc79e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36a2387a5814192b8147765f20fd962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ab89a9dff2645e492d14706357637dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 12\n",
      "['A2024253', '0854']\n",
      "['A2024253', '1030']\n",
      "['A2024253', '2012']\n",
      "['A2024253', '2154']\n",
      "['A2024254', '0830']\n",
      "['A2024254', '1012']\n",
      "['A2024254', '1954']\n",
      "['A2024254', '2130']\n",
      "['A2024255', '0954']\n",
      "['A2024255', '2112']\n",
      "['A2024256', '0936']\n",
      "['A2024256', '2054']\n",
      "NOAA20 -------\n",
      "Granules found: 13\n",
      "Opening 13 granules, approx size: 1.95 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b239ad6496044a3b517d4b9904e78a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b59ec46597041a7abe920fd7d3246d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da602201b2c044e3878e108dcb3d9711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 13\n",
      "Opening 13 granules, approx size: 2.21 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b0d020c53e48479fe407d51ccb6c11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aaf46fa046c4df98e21f8584c923ff2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b73a0d409c74ee19fd0a55e891c1ab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 13\n",
      "['A2024253', '0912']\n",
      "no data in scene\n",
      "['A2024253', '0918']\n",
      "['A2024253', '2036']\n",
      "['A2024254', '0854']\n",
      "['A2024254', '1036']\n",
      "['A2024254', '2018']\n",
      "['A2024255', '0836']\n",
      "['A2024255', '1018']\n",
      "['A2024255', '1954']\n",
      "['A2024255', '2136']\n",
      "['A2024256', '1000']\n",
      "['A2024256', '1936']\n",
      "['A2024256', '2118']\n",
      "NOAA21 -------\n",
      "Granules found: 9\n",
      "Opening 9 granules, approx size: 1.34 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f60c1811c0b841778d1ea447f63e6b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12409fe4f50b4e51a2d5bde0b7dc5a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29a76bb9fef4d4eb3f380f28f4fb01c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Granules found: 9\n",
      "Opening 9 granules, approx size: 1.63 GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68522832198b4f63a81071e957cb1a81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QUEUEING TASKS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f16da87c154468b9634c81e7dec0531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "PROCESSING TASKS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0465bcb0f574ed8b011fb9b35fb9e92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "COLLECTING RESULTS | :   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 files found\n",
      "['A2024253', '1006']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024253', '1948']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024253', '2124']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024254', '0948']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024254', '2106']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024255', '0930']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024255', '2048']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024256', '0912']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A2024256', '2030']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:129: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n",
      "<timed exec>:130: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "CPU times: user 2min 29s, sys: 43.3 s, total: 3min 12s\n",
      "Wall time: 24min\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "extent = BBOX #lon,lat,lon,lat\n",
    "\n",
    "for sat in products:\n",
    "    print(sat, '-------')\n",
    "\n",
    "    #FETCH DATA -----------------------------------------------------\n",
    "    files = {}\n",
    "\n",
    "    earthaccess.login(strategy='netrc') #for LAADS access - every hour\n",
    "\n",
    "    #Level1 data from LAADS ------\n",
    "    #geolocation 03IMG\n",
    "    results = earthaccess.search_data(\n",
    "        short_name=products[sat][0],\n",
    "        bounding_box=(extent[0],extent[1],extent[2],extent[3]),\n",
    "        temporal=(START, END),\n",
    "        count=800)\n",
    "    files[products[sat][0]] = earthaccess.open(results)\n",
    "\n",
    "    #science data 02IMG\n",
    "    results = earthaccess.search_data(\n",
    "        short_name=products[sat][1],\n",
    "        bounding_box=(extent[0],extent[1],extent[2],extent[3]),\n",
    "        temporal=(START, END),\n",
    "        count=800)\n",
    "    files[products[sat][1]] = earthaccess.open(results)\n",
    "\n",
    "    #Level2 14IMG data from LPDAAC ---------\n",
    "    if sat=='NOAA21':\n",
    "        urls = [r.data_links()[0] for r in results]\n",
    "        timestamps = ['.'.join(url.split('.')[-5:-3]) for url in urls]\n",
    "        timestamps\n",
    "\n",
    "        year = timestamps[0][1:5]\n",
    "        days = [t[5:8] for t in timestamps]\n",
    "        days = np.unique(days)\n",
    "\n",
    "        fls = []\n",
    "        for d in days:\n",
    "            direc = os.listdir(f'{input_dir}/VJ214IMG/{year}/{d}')\n",
    "            matches = [f for f in direc if any(t in f for t in timestamps)] #proud of this one\n",
    "            matches = [f'{ROOT}/viirs/VJ214IMG/{year}/{d}/{m}' for m in matches]\n",
    "            fls += matches\n",
    "\n",
    "        print(len(fls), 'files found') #should be same length as granules found for L1 products\n",
    "        files[products[sat][2]] = fls\n",
    "\n",
    "    else:\n",
    "        results = earthaccess.search_data(\n",
    "            short_name=products[sat][2],\n",
    "            bounding_box=(extent[0],extent[1],extent[2],extent[3]),\n",
    "            temporal=(START, END),\n",
    "            count=800)     \n",
    "        urls = [r.data_links(access='direct')[0] for r in results]\n",
    "        files[products[sat][2]] = [s3_fsspec.open(url) for url in urls]\n",
    "        #files[products[sat][2]] = earthaccess.open(results)\n",
    "\n",
    "    #pprint(files)\n",
    "    \n",
    "    #EXTRACT FIRE PIXELS -------------------------------------\n",
    "\n",
    "    #colormaps for plotting\n",
    "    mask_colors = [mpl.colormaps['tab10'](c) for c in [4,6,5,0,9,2,7,8,1,3]] #fire mask colors\n",
    "    dets_colors = ['white']*7 + ['black']*3                                  #black and white version\n",
    "\n",
    "    cmp1 = ListedColormap(mask_colors)\n",
    "    cmp2 = ListedColormap(dets_colors)\n",
    "\n",
    "    all_dets = pd.DataFrame() #list of all known+candidate detections per satellite\n",
    "\n",
    "    for i in range(len( files[products[sat][0]] )): #VNP03IMG or VJ103IMG\n",
    "        timestamp = files[products[sat][0]][i].path.split('.')[-5:-3]\n",
    "        print(timestamp)\n",
    "        year = timestamp[0][1:5]\n",
    "        day = timestamp[0][5:8]\n",
    "        time = timestamp[1]\n",
    "        date = dt.datetime.strptime(year+day, '%Y%j').strftime('%b %-d') \n",
    "        acq_datetime = dt.datetime.strptime(year+day+time[:2]+time[2:], '%Y%j%H%M')\n",
    "        #daytime = int(time) > 1500 #depends on timezone\n",
    "\n",
    "        try:\n",
    "            #open 03IMG geolocation\n",
    "            geo = xr.open_dataset(files[products[sat][0]][i], engine='h5netcdf', group='geolocation_data')\n",
    "            lon = geo['longitude'][:]\n",
    "            lat = geo['latitude'][:]\n",
    "            _, j = np.indices(geo.longitude.shape) #line and sample\n",
    "\n",
    "            scene = (lon > extent[0]) & (lon < extent[2]) & (lat > extent[1]) & (lat < extent[3])\n",
    "\n",
    "            #crop down the datasets for memory \n",
    "            indices = np.where(scene)\n",
    "            if len(indices[0])==0 or len(indices[1])==0:\n",
    "                print('no data in scene')\n",
    "                continue\n",
    "            x0 = indices[0].min()\n",
    "            x1 = indices[0].max()\n",
    "            y0 = indices[1].min()\n",
    "            y1 = indices[1].max()\n",
    "\n",
    "            lon = lon[x0:x1, y0:y1]\n",
    "            lat = lat[x0:x1, y0:y1]\n",
    "            j = j[x0:x1, y0:y1]\n",
    "            \n",
    "            if lon.size==0: continue #skip ahead\n",
    "\n",
    "            sza = geo['solar_zenith'].sel(number_of_lines=slice(x0,x1), number_of_pixels=slice(y0,y1))\n",
    "\n",
    "\n",
    "            #open 02IMG science data, i4 band\n",
    "            data = xr.open_dataset(files[products[sat][1]][i], engine='h5netcdf', group='observation_data')\n",
    "            data = data.sel(number_of_lines=slice(x0,x1), number_of_pixels=slice(y0,y1))\n",
    "\n",
    "            i4 = data['I04'] #xarray already encodes the scale factor and offset\n",
    "            scale = data.I04.encoding['scale_factor']\n",
    "            offset = data.I04.encoding['add_offset']\n",
    "            i4 = (i4[:,:] - offset) / scale #return to raw values to use lookup table to temperature\n",
    "            i4 = i4.astype(int)\n",
    "            i4_bt = data['I04_brightness_temperature_lut'][:]\n",
    "            i4_bt = i4_bt[i4]\n",
    "            \n",
    "            #get VNP14IMG\n",
    "            match = [f for f in files[products[sat][2]] if '.'.join([timestamp[0],timestamp[1]]) in str(f)][0]\n",
    "            if sat=='NOAA21': #local files\n",
    "                data = xr.open_dataset(match) #for some reason, xarray won't recognize phony dims on local files\n",
    "                dims = data.dims\n",
    "                dim1 = None\n",
    "                dim2 = None\n",
    "                for key in dims:\n",
    "                    if dims[key]==6400: dim2 = key\n",
    "                    elif dims[key] > 6400: dim1 = key\n",
    "                data = data.rename_dims({dim1:'dim1', dim2:'dim2'})\n",
    "                data = data.sel(dim1=slice(x0,x1), dim2=slice(y0,y1))\n",
    "            else: #s3 files\n",
    "                data = xr.open_dataset(match, phony_dims='sort')\n",
    "                data = data.sel(phony_dim_1=slice(x0,x1), phony_dim_2=slice(y0,y1))\n",
    "            \n",
    "            daynight = data.DayNightFlag #string Day or Night or Both\n",
    "\n",
    "            qa = data.variables['algorithm QA'][:]\n",
    "            fire = data.variables['fire mask'][:]  \n",
    "            fires = (fire>6).values\n",
    "\n",
    "        except:\n",
    "            print('error with file or does not exist',timestamp)\n",
    "            stop #code break - delete if working fine\n",
    "            continue\n",
    "    \n",
    "        #look at QA flags over entire scene\n",
    "        values, counts = np.unique(qa, return_counts=True)\n",
    "\n",
    "        table = pd.DataFrame(index = values, columns=range(22,-1,-1)) #[22,21,...0]\n",
    "        for i1 in table.index:\n",
    "            b = np.binary_repr(i1, width=23)\n",
    "            b = [int(s) for s in b]\n",
    "            table.loc[i1, :] = b\n",
    "\n",
    "        #report back all the pixels that have an 8 or 10 ~ background or candidate fires\n",
    "        keep = table[(table.loc[:,8]==1) | (table.loc[:,10]==1)].index\n",
    "        keep = (np.isin(qa[:], keep) | (fires))  #\"fires\" because some low conf are Test 16 pixel saturation\n",
    "\n",
    "        #build pandas table for exporting, following VIIRS L2 columns\n",
    "        i_dets = pd.DataFrame()\n",
    "        i_dets['longitude'] = list(lon.values[keep])\n",
    "        i_dets['latitude'] = list(lat.values[keep])\n",
    "        i_dets['fire_mask'] = list(fire.values[keep])\n",
    "        i_dets['confidence'] = i_dets.fire_mask\n",
    "        i_dets.confidence = i_dets.confidence.replace({0:'x', 1:'x', 2:'x', 3:'x', 4:'x', 5:'x', 6:'x', 7:'l', 8:'n', 9:'h'})\n",
    "        i_dets['acq_date'] = acq_datetime.strftime('%Y-%m-%d') \n",
    "        i_dets['acq_time'] = acq_datetime.strftime('%H:%M') \n",
    "        i_dets['acq_datetime'] = acq_datetime.strftime('%Y-%m-%d %H:%M:00 +00:00') \n",
    "        i_dets['j'] = list(j[keep]) #sample number for pixel size lookup\n",
    "        i_dets['vza'] = pix_lut.loc[i_dets['j'], 'scan_angle'].values\n",
    "        i_dets['sza'] = sza.values[keep]\n",
    "        i_dets['daynight'] = np.where(i_dets.sza<90, 'D', 'N') #day where SZA<90 deg\n",
    "\n",
    "        #crop down to defined extent\n",
    "        i_dets = i_dets[(i_dets.longitude > extent[0]) & (i_dets.longitude < extent[2]) & (i_dets.latitude > extent[1]) & (i_dets.latitude < extent[3])]\n",
    "\n",
    "        knowns_count = (i_dets.fire_mask > 6).sum()\n",
    "        cands_count = (i_dets.fire_mask < 7).sum()\n",
    "        \n",
    "    \n",
    "        #FIGURES (optional) -------------------------------------------\n",
    "        '''\n",
    "        fig, ((ax,ax2,ax3,ax4),(ax5,ax6,ax7,ax8)) = plt.subplots(2,4, gridspec_kw={'width_ratios':[3,3,3,1], 'height_ratios':[6,1]}, constrained_layout=True, subplot_kw={'projection':ccrs.Miller()}, figsize=(12,8))\n",
    "\n",
    "        #Level 1 imagery\n",
    "        ax.set_extent([extent[0],extent[2],extent[1],extent[3]])\n",
    "        plot = ax.pcolormesh(lon, lat, i4_bt, vmin=250, vmax=360, cmap='plasma', transform=ccrs.PlateCarree())\n",
    "        cbar = plt.colorbar(plot, orientation='horizontal', shrink=0.6, pad=-2.4, extend='both', ax=ax5)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        cbar.set_label('I4 brightness temperature (K)', size=12)\n",
    "\n",
    "        #Level 1 imagery plus detections\n",
    "        ax2.set_extent([extent[0],extent[2],extent[1],extent[3]])\n",
    "        plot = ax2.pcolormesh(lon, lat, i4_bt, vmin=250, vmax=360, cmap='plasma', transform=ccrs.PlateCarree())\n",
    "        cbar = plt.colorbar(plot, orientation='horizontal', shrink=0.6, pad=-2.4, extend='both', ax=ax6)\n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "        cbar.set_label('I4 brightness temperature (K)', size=12)\n",
    "        ax2.set_title(f'{sat} {date} {time}h UTC')\n",
    "\n",
    "        ax2.scatter(i_dets.longitude, i_dets.latitude, c=cmp2(i_dets['fire_mask'].astype(int)), s=0.5, transform=ccrs.Geodetic())\n",
    "        ax2.text(0.1, 0.92, f'{knowns_count} known fire pixels', c='black', transform = ax2.transAxes, fontsize=12)\n",
    "        ax2.text(0.1, 0.87, f'{cands_count} candidate fire pixels', c='white', transform = ax2.transAxes, fontsize=12)\n",
    "\n",
    "        #Level 2 fire mask\n",
    "        ax3.set_extent([extent[0],extent[2],extent[1],extent[3]])\n",
    "        plot = ax3.pcolormesh(lon, lat, fire, vmin=0, vmax=10, cmap=cmp1, transform=ccrs.PlateCarree())\n",
    "\n",
    "        #Level 2 fire mask legend\n",
    "        cbar = plt.colorbar(plot, orientation='vertical', shrink=0.8, pad=-1, ax=ax4)\n",
    "\n",
    "        labels = ['0 not-processed', '1 bowtie', '2 glint', '3 water','4 clouds',\n",
    "              '5 clear land','6 unclassified fire pixel','7 low confidence fire pixel',\n",
    "              '8 nominal confidence fire pixel','9 high confidence fire pixel']\n",
    "        cbar.ax.set_yticks(np.arange(len(labels))+0.5)\n",
    "        cbar.ax.set_yticklabels(labels) \n",
    "        cbar.ax.tick_params(labelsize=12)\n",
    "\n",
    "        ax4.axis('off')\n",
    "        ax5.axis('off')\n",
    "        ax6.axis('off')\n",
    "        ax7.axis('off')\n",
    "        ax8.axis('off')\n",
    "        \n",
    "        #save fig - specify fire/folder name\n",
    "        plt.savefig(f'{output_dir}/outputs/{NAME}/{timestamp[0]}-{timestamp[1]}_{sat}.png', dpi=125, bbox_inches='tight')\n",
    "        plt.close() \n",
    "        '''\n",
    "        #----------------------------------\n",
    "        \n",
    "        all_dets = pd.concat([all_dets, i_dets])\n",
    "        \n",
    "        del geo, scene, data, lon, lat, i4, i_dets\n",
    "\n",
    "    #save csv of detections - specify fire/folder name\n",
    "    all_dets.to_csv(f'{output_dir}/outputs/{NAME}/detections_{sat}.csv', index=False)\n",
    "    \n",
    "    del results, files, all_dets\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33acf9ae-9fbb-4bc9-8832-d41ff887c80a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pangeo] *",
   "language": "python",
   "name": "conda-env-pangeo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
